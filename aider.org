:PROPERTIES:
:ID:       b5c02b4f-4476-4af1-88ad-2ca1cd2aec8e
:END:
#+title: Karpor-MCP
#+filetags: :open-source:project:

* Context
** Github Repository
  https://github.com/KusionStack/karpor
** Documentation
  https://www.kusionstack.io/karpor/
** Issue
 - https://github.com/KusionStack/karpor/issues/658
** Discussion
 - https://github.com/KusionStack/karpor/discussions/675
** Pull Request
 - https://github.com/KusionStack/karpor/pull/798
** Code Contribution Guidelines
 - https://www.kusionstack.io/karpor/developer-guide/conventions/code-conventions
** Testing strategy
 - https://www.kusionstack.io/karpor/developer-guide/conventions/test-conventions
* Libraries
** mcp-go
 - https://github.com/mark3labs/mcp-go
* env Setup
** kind installation
 - https://kind.sigs.k8s.io/
** karpor quick start
 - https://www.kusionstack.io/karpor/getting-started/quick-start
** register local cluster
 - https://www.kusionstack.io/karpor/getting-started/quick-start#register-cluster
** karpor's architecture
 - https://www.kusionstack.io/karpor/concepts/architecture
* Code Overview
** hack
  The `hack` directory contains scripts and tools for automating development tasks, code generation, and verification.
  - **Code Generation:** Scripts (`update-codegen.sh`, `generate-groups.sh`, `generate-internal-groups.sh`) generate Go code for Kubernetes-style APIs (deepcopy, clientsets, listers, informers, defaulters, conversions, openapi). Uses tools from `tools.go` and boilerplate headers.
  - **Code Verification:** `verify-codegen.sh` checks if generated code is up-to-date.
  - **CLI Documentation Generation:** `gen-cli-docs/main.go` generates Markdown documentation for Karpor CLI commands.
  - **Utility Functions:** `util.sh` provides helper functions like setting up a temporary GOPATH.
** cmd
  The `cmd` directory contains the entry points for the main Karpor executable applications. Each subdirectory under `cmd` represents a distinct binary.
  - **`cmd/karpor`**: This is the main Karpor application binary (`cmd/karpor/main.go`). It acts as a command-line interface multiplexer using `cobra`, allowing users to run different components via subcommands:
    - `karpor server`: Starts the core Karpor API server (`cmd/karpor/app/server.go`). Provides a Kubernetes-style API, handles authentication, authorization (RBAC), storage (Elasticsearch), and serves OpenAPI docs. Configured via options in `cmd/karpor/app/options/`.
    - `karpor syncer`: Starts the resource syncer component (`cmd/karpor/app/syncer.go`). Connects to Kubernetes clusters and synchronizes resource data into a search storage backend (Elasticsearch). Uses `controller-runtime`.
    - `karpor mcp`: Intended to start a server for natural language interaction (`cmd/karpor/app/mcp.go`). Currently under development, initializes an Elasticsearch client but the core SSE server and interaction logic are pending implementation.
  - **`cmd/cert-generator`**: A separate utility binary (`cmd/cert-generator/main.go`). Generates CA certificates and kubeconfig files required for Karpor components to interact securely with Kubernetes clusters. Stores credentials as Kubernetes Secrets and ConfigMaps.

** api
  The `api` directory is dedicated to defining and serving the OpenAPI (Swagger) specification for the Karpor Core API.
  - **API Specification Definition:** The `openapispec` subdirectory contains the formal OpenAPI specification files (`swagger.yaml` and `swagger.json`). These files describe the API endpoints, methods, parameters, responses, and data structures.
  - **Generated Code for Serving the Spec:** `api/openapispec/docs.go` is a generated Go file that embeds the OpenAPI specification within the Karpor binary, allowing the API server to serve the spec programmatically (e.g., for Swagger UI).
  - **Documentation:** `README.md` files provide context and documentation for the API definition files.

** .github
  The `.github` directory contains configuration files for GitHub features, primarily focusing on automated workflows and contribution guidelines.
  - **Workflows (`.github/workflows/`)**: Defines Continuous Integration (CI), release automation, and community management tasks using GitHub Actions.
    - `check.yaml`: Runs unit tests, Go linting, and license checks on pull requests and pushes to `main`.
    - `constraint.yaml`: Enforces pull request title conventions and checks for broken links in Markdown files.
    - `release.yaml`: Automates the release process upon tag creation, including building artifacts, pushing container images, and updating the Helm chart repository.
    - `cla.yaml`: Manages the Contributor License Agreement (CLA) signing process for pull requests.
    - `community-task-updater.yml`: Uses the `osp-action` to manage community tasks based on issue events.
    - `community-planning-updater.yml`: Uses the `osp-action` to manage community planning based on milestone and issue events.
  - **Issue and Pull Request Templates (`.github/ISSUE_TEMPLATE/`, `.github/PULL_REQUEST_TEMPLATE.md`)**: Provides standardized templates for reporting bugs, suggesting enhancements, and creating pull requests.
  - **CODEOWNERS**: Specifies individuals or teams responsible for reviewing code in different parts of the repository.
* Plan
** Phase 1: Core mcp-go Integration with Elasticsearch (Basic Resource)
*** Goal: Get a minimal MCP SSE server running that exposes one type of resource fetched from Elasticsearch.
*** Study:
**** Deeply understand the `mcp-go` library:
***** Focus on `mcp.Server`, `server.SSEServer`.
***** Understand the `mcp.Resource`, `mcp.Tool`, `mcp.Prompt` interfaces and how they are registered and used by the server.
***** Review `mcp-go` examples if available.
**** Review Karpor's Elasticsearch storage implementation (`pkg/infra/search/storage/elasticsearch`):
***** How are resources queried and retrieved?
***** What is the structure of stored data?
**** Analyze Karpor's core entity structures (`pkg/core/entity`):
***** Choose one simple entity (e.g., `ResourceGroup`) to expose first.
**** Re-read Issue #658 and Discussion #675 for specific requirements or use cases.
**** Review the current state of PR #798.
*** Design:
**** Refine the `pkg/mcp` package structure.
**** Design a struct that implements the `mcp.Resource` interface for the chosen Karpor entity (e.g., `ResourceGroup`). This struct will need access to the Elasticsearch storage client.
**** Determine how to map Karpor's entity data to the structure expected by the `mcp.Resource` interface methods (e.g., `List`, `Get`).
**** Outline the necessary modifications in `pkg/mcp/server.go` to initialize the `mcp-go` server and register the implemented `mcp.Resource`.
**** Plan the final integration steps in `cmd/karpor/app/mcp.go` to create the storage client, create the `MCPStorageServer`, and start it.
*** Programming:
**** Implement the `mcp.Resource` interface methods (`List`, `Get`, etc.) in a new file/struct within `pkg/mcp` (e.g., `pkg/mcp/resourcegroup.go`).
***** Inside these methods, use the Karpor Elasticsearch storage client to fetch data.
***** Handle potential errors from the storage layer.
**** Update `pkg/mcp/server.go`:
***** Modify `NewMCPStorageServer` to accept and store the `storage.Storage` interface.
***** Add a method (e.g., `RegisterResources`) to register the implemented `mcp.Resource` instances with the internal `mcpServer`.
**** Update `cmd/karpor/app/mcp.go`:
***** After initializing the Elasticsearch storage, call the registration method on the `MCPStorageServer` instance before calling `Serve`.
***** Ensure proper context propagation and error handling during server startup.
***** Add basic logging using the configured `klogr` logger.
***** Address any linter warnings (`nolint` comments should be reviewed).
*** Testing:
**** Write unit tests for the `mcp.Resource` implementation, mocking the storage backend.
**** Write integration tests for the server startup and the basic resource listing endpoint.
**** Manual testing: Run the `karpor mcp` command and use a tool (like `curl` or a simple client) to connect to the SSE endpoint and verify that resource events are received.
*** Other:
**** Ensure `go.mod` and `go.sum` are clean and correct.
**** Set up a local Elasticsearch instance for testing if not already available.

** Phase 2: Expanding Resource Coverage & Basic Tools/Prompts (Elasticsearch)
*** Goal: Expose more Karpor entities as MCP Resources and implement basic MCP Tools and Prompts interacting with Elasticsearch data.
*** Study:
**** Identify other critical Karpor entities to expose (e.g., `ResourceGroupRule`).
**** Understand the `mcp.Tool` and `mcp.Prompt` interfaces in detail.
**** How can Karpor's existing logic (e.g., AI manager) be integrated via the `mcp.Prompt` interface?
*** Design:
**** Design implementations for additional `mcp.Resource` types.
**** Design one or two simple `mcp.Tool` implementations (e.g., an action related to a resource).
**** Design one or two simple `mcp.Prompt` implementations (e.g., asking a question about a resource group count).
**** Plan how these new implementations will be registered with the `mcp-go` server.
*** Programming:
**** Implement `mcp.Resource` for additional entities.
**** Implement `mcp.Tool` for selected actions, interacting with storage or other Karpor managers as needed.
**** Implement `mcp.Prompt` for selected AI interactions, integrating with Karpor's AI manager if applicable.
**** Update `pkg/mcp/server.go` to register these new Resources, Tools, and Prompts.
**** Write unit tests for all new implementations.
*** Testing:
**** Write integration tests for the new Resources, Tools, and Prompts.
**** Manual testing of all exposed capabilities.

** Phase 3: Etcd Integration (If Required)
*** Goal: Integrate Etcd as a potential data source for MCP, if Karpor uses Etcd for data relevant to MCP.
*** Study:
**** Determine if Karpor currently uses Etcd for data that should be exposed via MCP.
**** If so, understand Karpor's Etcd client and data structures.
**** How can `mcp-go` interfaces (`Resource`, `Tool`, `Prompt`) be implemented using Etcd as a backend?
*** Design:
**** Design Etcd-specific implementations of `mcp.Resource`, `mcp.Tool`, `mcp.Prompt` if the data source requires it.
**** Plan how the server setup will handle potentially multiple storage backends (Elasticsearch and Etcd).
*** Programming:
**** Implement Etcd-backed MCP interfaces.
**** Update server registration logic to include Etcd-backed components.
**** Write unit tests.
*** Testing:
**** Write integration tests for Etcd-backed features.
**** Manual testing with Etcd backend.

** Phase 4: Refinement, Comprehensive Testing, and Documentation
*** Goal: Ensure the MCP server is robust, well-tested, and documented.
*** Programming:
**** Conduct thorough code reviews.
**** Refactor code for clarity, maintainability, and performance.
**** Improve error handling, logging, and metrics (if needed).
**** Address any remaining linter issues.
*** Testing:
**** Run the full suite of unit, integration, and potentially end-to-end tests.
**** Performance testing if necessary.
**** Address any bugs found.
*** Documentation:
**** Document the MCP server's purpose, configuration options, and how to run it.
**** Document the specific Resources, Tools, and Prompts exposed by the Karpor MCP server.
**** Update relevant READMEs and Karpor documentation.
*** Other:
**** Prepare the changes for merging (squash commits, write clear commit messages).
**** Coordinate with team for final review and merge.

* Misc Notes
** Leverage existing natural language search mechanism
** karpor mcp is a separate command just like syncer
 - is init by executing karpor mcp : starts the mcp server
* Misc TODO
 - [ ] get a basic `karpor mcp` server running
* Aider Directives
** TDD : Test Driven Development
 - https://martinfowler.com/bliki/TestDrivenDevelopment.html
 - testify for unit tests
 - ginkgo and gomega for integration and end to end tests

* TDD Scratch pad
 - need to catch up on tests
 - testify for unit tests and ginkgo, gomega for the integration and end to end tests
 - my scope can all be restricted to karpor mcp working right
 - I don't need to check the functionality of cobra or viper working well but need to restrict myself to the specifics of mcp-go and  how it fits

:PROPERTIES:
:ID:       b5c02b4f-4476-4af1-88ad-2ca1cd2aec8e
:END:
#+title: Karpor-MCP
#+filetags: :open-source:project:

* Context
** Github Repository
  https://github.com/KusionStack/karpor
** Documentation
  https://www.kusionstack.io/karpor/
** Issue
 - https://github.com/KusionStack/karpor/issues/658
** Discussion
 - https://github.com/KusionStack/karpor/discussions/675
** Pull Request
 - https://github.com/KusionStack/karpor/pull/798
** Code Contribution Guidelines
 - https://www.kusionstack.io/karpor/developer-guide/conventions/code-conventions
** Testing strategy
 - https://www.kusionstack.io/karpor/developer-guide/conventions/test-conventions
* Libraries
** mcp-go
 - https://github.com/mark3labs/mcp-go
*** Overview
  The `mcp-go` library is a Go implementation of the Model Context Protocol (MCP). It enables building servers that expose data and functionality to LLM applications in a secure, standardized way. It acts as a bridge between LLMs and external data sources or tools.

  Key Concepts:
  - **Server:** The core component (`server.MCPServer`) that handles connection management, protocol compliance, and routes messages.
  - **Resources:** Used to expose data to LLMs (like read-only GET endpoints). Can be static or dynamic (using URI templates). You define a Resource with a URI, name, description, MIME type, and a handler function to fetch data.
  - **Tools:** Allow LLMs to perform actions or computations (like POST endpoints with side effects). You define a Tool with a name, description, arguments (with types and constraints), and a handler function to execute the logic and return a result.
  - **Prompts:** Reusable templates for structuring interactions with LLMs. They can include system instructions, required arguments, embedded Resources, and multiple messages to guide the LLM.

  Key Features:
  - **Fast & Simple:** High-level interface reduces boilerplate.
  - **Complete:** Aims to provide a full implementation of the core MCP specification (under active development).
  - **Session Management:** Supports maintaining per-client state, notifications, and per-session tool customization/filtering.
  - **Request Hooks:** Allows hooking into the request lifecycle for observability.
  - **Tool Handler Middleware:** Enables adding middleware to tool handlers (e.g., recovery).

  Development Status:
  - The library and the MCP specification are under active development. Core features are working, but some advanced capabilities are still in progress.

* env Setup
** kind installation
 - https://kind.sigs.k8s.io/
** karpor quick start
 - https://www.kusionstack.io/karpor/getting-started/quick-start
** register local cluster
 - https://www.kusionstack.io/karpor/getting-started/quick-start#register-cluster
** karpor's architecture
 - https://www.kusionstack.io/karpor/concepts/architecture
* Code Overview
** hack
  The `hack` directory contains scripts and tools for automating development tasks, code generation, and verification.
  - **Code Generation:** Scripts (`update-codegen.sh`, `generate-groups.sh`, `generate-internal-groups.sh`) generate Go code for Kubernetes-style APIs (deepcopy, clientsets, listers, informers, defaulters, conversions, openapi). Uses tools from `tools.go` and boilerplate headers.
  - **Code Verification:** `verify-codegen.sh` checks if generated code is up-to-date.
  - **CLI Documentation Generation:** `gen-cli-docs/main.go` generates Markdown documentation for Karpor CLI commands.
  - **Utility Functions:** `util.sh` provides helper functions like setting up a temporary GOPATH.
** cmd
  The `cmd` directory contains the entry points for the main Karpor executable applications. Each subdirectory under `cmd` represents a distinct binary.
  - **`cmd/karpor`**: This is the main Karpor application binary (`cmd/karpor/main.go`). It acts as a command-line interface multiplexer using `cobra`, allowing users to run different components via subcommands:
    - `karpor server`: Starts the core Karpor API server (`cmd/karpor/app/server.go`). Provides a Kubernetes-style API, handles authentication, authorization (RBAC), storage (Elasticsearch), and serves OpenAPI docs. Configured via options in `cmd/karpor/app/options/`.
    - `karpor syncer`: Starts the resource syncer component (`cmd/karpor/app/syncer.go`). Connects to Kubernetes clusters and synchronizes resource data into a search storage backend (Elasticsearch). Uses `controller-runtime`.
    - `karpor mcp`: Intended to start a server for natural language interaction (`cmd/karpor/app/mcp.go`). Currently under development, initializes an Elasticsearch client but the core SSE server and interaction logic are pending implementation.
  - **`cmd/cert-generator`**: A separate utility binary (`cmd/cert-generator/main.go`). Generates CA certificates and kubeconfig files required for Karpor components to interact securely with Kubernetes clusters. Stores credentials as Kubernetes Secrets and ConfigMaps.
** api
  The `api` directory is dedicated to defining and serving the OpenAPI (Swagger) specification for the Karpor Core API.
  - **API Specification Definition:** The `openapispec` subdirectory contains the formal OpenAPI specification files (`swagger.yaml` and `swagger.json`). These files describe the API endpoints, methods, parameters, responses, and data structures.
  - **Generated Code for Serving the Spec:** `api/openapispec/docs.go` is a generated Go file that embeds the OpenAPI specification within the Karpor binary, allowing the API server to serve the spec programmatically (e.g., for Swagger UI).
  - **Documentation:** `README.md` files provide context and documentation for the API definition files.
** .github
  The `.github` directory contains configuration files for GitHub features, primarily focusing on automated workflows and contribution guidelines.
  - **Workflows (`.github/workflows/`)**: Defines Continuous Integration (CI), release automation, and community management tasks using GitHub Actions.
    - `check.yaml`: Runs unit tests, Go linting, and license checks on pull requests and pushes to `main`.
    - `constraint.yaml`: Enforces pull request title conventions and checks for broken links in Markdown files.
    - `release.yaml`: Automates the release process upon tag creation, including building artifacts, pushing container images, and updating the Helm chart repository.
    - `cla.yaml`: Manages the Contributor License Agreement (CLA) signing process for pull requests.
    - `community-task-updater.yml`: Uses the `osp-action` to manage community tasks based on issue events.
    - `community-planning-updater.yml`: Uses the `osp-action` to manage community planning based on milestone and issue events.
  - **Issue and Pull Request Templates (`.github/ISSUE_TEMPLATE/`, `.github/PULL_REQUEST_TEMPLATE.md`)**: Provides standardized templates for reporting bugs, suggesting enhancements, and creating pull requests.
  - **CODEOWNERS**: Specifies individuals or teams responsible for reviewing code in different parts of the repository.
** .goreleaser
  The `.goreleaser` directory contains configuration files for the Goreleaser tool, which automates the process of building and releasing Go projects.
  - **`.goreleaser.yml`**: The main configuration file used for official releases. It defines build targets (OS/Arch), archives, checksums, changelog generation, GitHub releases, and Docker image builds (including multi-platform manifests).
  - **`.goreleaser-dev.yml`**: A configuration file intended for development purposes. It typically disables the release step (`release: disable: true`) while still allowing builds and Docker image creation, useful for local testing of the build process without creating official releases.
** config
  The `config` directory contains configuration file templates and default configurations for Karpor components. These files are often embedded into the binary.
  - **`config/README.md`**: Provides a brief description of the directory's purpose.
  - **`config/embed.go`**: A Go file that uses `//go:embed` directives to embed the default configuration YAML files into the Karpor binary. It defines variables holding the byte content of these files.
  - **`config/default-anonymous-rbac.yaml`**: Defines a Kubernetes `ClusterRole` and `ClusterRoleBinding` for the `system:anonymous` user, granting `get` access to public API endpoints.
  - **`config/default-karpor-admin-rbac.yaml`**: Defines a Kubernetes `ClusterRole` named `karpor-admin` with broad access (`*` verb) to resource group and cluster APIs, and `get` access to public endpoints.
  - **`config/default-karpor-guest-rbac.yaml`**: Defines a Kubernetes `ClusterRole` named `karpor-guest` with `get` access to various public and resource-related API endpoints.
  - **`config/default-relationship.yaml`**: Defines default relationships between Kubernetes resources (e.g., Deployment -> ReplicaSet -> Pod) using selectors, owner references, and JSONPath, used for graph traversal and visualization.
  - **`config/default-sync-strategy.yaml`**: Defines default `TransformRule` and `SyncRegistry` configurations for the Karpor syncer, specifying which resources to synchronize from clusters and how to transform their data before storing it (e.g., only keeping metadata).
** pkg
*** readme
  The `pkg` directory contains library code intended for use by external applications. While Go's `internal` directory enforces privacy, `pkg` explicitly communicates that the code within is safe for others to import. It helps organize Go code when the root directory is busy with non-Go components, making it easier to use Go tools. The pattern is not universally accepted but is common in many large Go projects (like Kubernetes, Helm, Docker, etc.), originating from the old Go source code structure. It's most beneficial for larger projects where the extra nesting aids organization.
*** mcp
  The `pkg/mcp` directory contains the implementation for the Karpor MCP (Multi-Cluster Platform) server component. This component is designed to expose Karpor's data and capabilities, particularly from the search storage backend, via the `mcp-go` library's Server-Sent Events (SSE) interface.
  - **`types.go`**: Defines the core types used within the MCP package, including `MCPStorageServer` (the main struct holding the storage backend and `mcp-go` server instances) and type aliases for naming Resources, Tools, and Prompts.
  - **`server.go`**: Contains the logic for initializing and starting the `mcp-go` SSE server. It includes the `NewMCPStorageServer` function to create the server instance with a storage backend and SSE configuration, and the `Serve` method to start the SSE listener.
  - **`resources.go`**: Intended to contain implementations of the `mcp-go/mcp.Resource` interface. These implementations will define how specific Karpor entities (like `ResourceGroup`) are exposed and queried via the MCP server, likely interacting with the configured storage backend. (Currently empty, pending implementation).
  - **`tools.go`**: Intended to contain implementations of the `mcp-go/mcp.Tool` interface. These implementations will define actions or operations that the MCP server can perform, potentially interacting with Karpor's managers or storage. (Currently empty, pending implementation).
  - **`prompts.go`**: Intended to contain implementations of the `mcp-go/mcp.Prompt` interface. These implementations will define natural language interaction capabilities, potentially integrating with Karpor's AI manager or querying data via the storage backend. (Currently empty, pending implementation).
* Plan
** Phase 1: Core mcp-go Integration with Elasticsearch (Basic Resource)
*** Goal: Get a minimal MCP SSE server running that exposes one type of resource fetched from Elasticsearch.
*** Study:
**** Deeply understand the `mcp-go` library:
***** Focus on `mcp.Server`, `server.SSEServer`.
***** Understand the `mcp.Resource`, `mcp.Tool`, `mcp.Prompt` interfaces and how they are registered and used by the server.
***** Review `mcp-go` examples if available.
**** Review Karpor's Elasticsearch storage implementation (`pkg/infra/search/storage/elasticsearch`):
***** How are resources queried and retrieved?
***** What is the structure of stored data?
**** Analyze Karpor's core entity structures (`pkg/core/entity`):
***** Choose one simple entity (e.g., `ResourceGroup`) to expose first.
**** Re-read Issue #658 and Discussion #675 for specific requirements or use cases.
**** Review the current state of PR #798.
*** Design:
**** Refine the `pkg/mcp` package structure.
**** Design a struct that implements the `mcp.Resource` interface for the chosen Karpor entity (e.g., `ResourceGroup`). This struct will need access to the Elasticsearch storage client.
**** Determine how to map Karpor's entity data to the structure expected by the `mcp.Resource` interface methods (e.g., `List`, `Get`).
**** Outline the necessary modifications in `pkg/mcp/server.go` to initialize the `mcp-go` server and register the implemented `mcp.Resource`.
**** Plan the final integration steps in `cmd/karpor/app/mcp.go` to create the storage client, create the `MCPStorageServer`, and start it.
*** Programming:
**** Implement the `mcp.Resource` interface methods (`List`, `Get`, etc.) in a new file/struct within `pkg/mcp` (e.g., `pkg/mcp/resourcegroup.go`).
***** Inside these methods, use the Karpor Elasticsearch storage client to fetch data.
***** Handle potential errors from the storage layer.
**** Update `pkg/mcp/server.go`:
***** Modify `NewMCPStorageServer` to accept and store the `storage.Storage` interface.
***** Add a method (e.g., `RegisterResources`) to register the implemented `mcp.Resource` instances with the internal `mcpServer`.
***** Ensure proper context propagation and error handling during server startup.
***** Add basic logging using the configured `klogr` logger.
***** Address any linter warnings (`nolint` comments should be reviewed).
**** Update `cmd/karpor/app/mcp.go`:
***** After initializing the Elasticsearch storage, call the registration method on the `MCPStorageServer` instance before calling `Serve`.
*** Testing:
**** Write unit tests for the `mcp.Resource` implementation, mocking the storage backend.
**** Write integration tests for the server startup and the basic resource listing endpoint.
**** Manual testing: Run the `karpor mcp` command and use a tool (like `curl` or a simple client) to connect to the SSE endpoint and verify that resource events are received.
*** Other:
**** Ensure `go.mod` and `go.sum` are clean and correct.
**** Set up a local Elasticsearch instance for testing if not already available.

** Phase 2: Expanding Resource Coverage & Basic Tools/Prompts (Elasticsearch)
*** Goal: Expose more Karpor entities as MCP Resources and implement basic MCP Tools and Prompts interacting with Elasticsearch data.
*** Study:
**** Identify other critical Karpor entities to expose (e.g., `ResourceGroupRule`).
**** Understand the `mcp.Tool` and `mcp.Prompt` interfaces in detail.
**** How can Karpor's existing logic (e.g., AI manager) be integrated via the `mcp.Prompt` interface?
*** Design:
**** Design implementations for additional `mcp.Resource` types.
**** Design one or two simple `mcp.Tool` implementations (e.g., an action related to a resource).
**** Design one or two simple `mcp.Prompt` implementations (e.g., asking a question about a resource group count).
**** Plan how these new implementations will be registered with the `mcp-go` server.
*** Programming:
**** Implement `mcp.Resource` for additional entities.
**** Implement `mcp.Tool` for selected actions, interacting with storage or other Karpor managers as needed.
**** Implement `mcp.Prompt` for selected AI interactions, integrating with Karpor's AI manager if applicable.
**** Update `pkg/mcp/server.go` to register these new Resources, Tools, and Prompts.
**** Write unit tests for all new implementations.
*** Testing:
**** Write integration tests for the new Resources, Tools, and Prompts.
**** Manual testing of all exposed capabilities.

** Phase 3: Etcd Integration (If Required)
*** Goal: Integrate Etcd as a potential data source for MCP, if Karpor uses Etcd for data relevant to MCP.
*** Study:
**** Determine if Karpor currently uses Etcd for data that should be exposed via MCP.
**** If so, understand Karpor's Etcd client and data structures.
**** How can `mcp-go` interfaces (`Resource`, `Tool`, `Prompt`) be implemented using Etcd as a backend?
*** Design:
**** Design Etcd-specific implementations of `mcp.Resource`, `mcp.Tool`, `mcp.Prompt` if the data source requires it.
**** Plan how the server setup will handle potentially multiple storage backends (Elasticsearch and Etcd).
*** Programming:
**** Implement Etcd-backed MCP interfaces.
**** Update server registration logic to include Etcd-backed components.
**** Write unit tests.
*** Testing:
**** Write integration tests for Etcd-backed features.
**** Manual testing with Etcd backend.

** Phase 4: Refinement, Comprehensive Testing, and Documentation
*** Goal: Ensure the MCP server is robust, well-tested, and documented.
*** Programming:
**** Conduct thorough code reviews.
**** Refactor code for clarity, maintainability, and performance.
**** Improve error handling, logging, and metrics (if needed).
**** Address any remaining linter issues.
*** Testing:
**** Run the full suite of unit, integration, and potentially end-to-end tests.
**** Performance testing if necessary.
**** Address any bugs found.
*** Documentation:
**** Document the MCP server's purpose, configuration options, and how to run it.
**** Document the specific Resources, Tools, and Prompts exposed by the Karpor MCP server.
**** Update relevant READMEs and Karpor documentation.
*** Other:
**** Prepare the changes for merging (squash commits, write clear commit messages).
**** Coordinate with team for final review and merge.

* Core Design Notes
** Purpose Overview
  The **MCP Server** in Karpor is being developed to enable a **natural language chat interface** for interacting with Karpor's stored Kubernetes cluster data.

  Its primary purpose is to act as an intermediary that:

  1.  **Implements the Model Context Protocol (MCP):** It provides a standardized interface (using libraries like `mcp-go`) for communication with AI models (like Large Language Models - LLMs).
  2.  **Integrates with Karpor Storage:** It connects to Karpor's backend storage (specifically Elasticsearch, as mentioned in the comments) to access the synchronized Kubernetes resource data.
  3.  **Translates Natural Language:** It receives natural language queries from a user interface (the planned chat interface) and translates them into structured requests that can be executed against Karpor's storage or other Karpor components.
  4.  **Provides Intelligent Responses:** It processes the results from Karpor's data sources and formats them according to the MCP standard, allowing AI models to consume this context and generate natural language responses, insights, or recommendations for the user.

  In essence, the MCP server aims to make Karpor's rich multi-cluster Kubernetes data accessible and understandable through intuitive natural language interactions, leveraging AI capabilities via a standardized protocol.
* Specific Design Notes
** Leverage existing natural language search mechanism
** karpor mcp is a separate command just like syncer
 - is init by executing karpor mcp : starts the mcp server
 - cobra + viper
** Overall Purpose of elastic search mcp
* Aider Directives
** TDD : Test Driven Development
 - https://martinfowler.com/bliki/TestDrivenDevelopment.html
 - testify for unit tests
 - ginkgo and gomega for integration and end to end tests
* Human Thoughts
** Iterative runs
 - make -j 16 build-linux
 - karpor mcp , fails properly due to inestablished elastic search servers
 - need to decide order of spawning the karpor server, syncer, elasticsearch and etcd
 - containerized versions of the tests should be available
 - need to establish how the runs work
 - unit tests from the get go for now
 - Not the ideal behaviour, the server should be spawned even if the storage backend isn't present

** multiple storage backends
 - moved from using a single elasticsearch client to multiple storage backends ([]storage.Storage)
 - need a way to receive and manage those backends in the larger control flow
 - all of this is orchestrated in cmd .. mcp.go run
 - need to readup on the storage.Storage interface
